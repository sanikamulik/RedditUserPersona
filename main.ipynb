{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1a65cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import openai\n",
    "import os\n",
    "from getpass import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e884d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "from getpass import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b514a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de2f61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['REDDIT_CLIENT_ID'] = getpass('Enter your Reddit client ID: ')\n",
    "os.environ['REDDIT_CLIENT_SECRET'] = getpass('Enter your Reddit client secret: ')\n",
    "os.environ['REDDIT_USER_AGENT'] = getpass('Enter your Reddit user agent: ')\n",
    "os.environ['GOOGLE_API_KEY'] = getpass('Enter your Google AI API Key: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1e85d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    reddit = praw.Reddit(\n",
    "        client_id=os.environ['REDDIT_CLIENT_ID'],\n",
    "        client_secret=os.environ['REDDIT_CLIENT_SECRET'],\n",
    "        user_agent=os.environ['REDDIT_USER_AGENT']\n",
    "    )\n",
    "    next(reddit.front.hot(limit=1))\n",
    "    print(\"Successfully connected to Reddit API.\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to connect to Reddit API. Please check your credentials. Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e00f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_redditor_data(username):\n",
    "    try:\n",
    "        redditor = reddit.redditor(username)\n",
    "        if not hasattr(redditor, 'id'):\n",
    "            print(f\"User '{username}' not found or is suspended.\")\n",
    "            return None\n",
    "\n",
    "        comments = []\n",
    "        for comment in redditor.comments.new(limit=None):\n",
    "            comments.append(f\"Comment in r/{comment.subreddit.display_name} (Score: {comment.score}): {comment.body}\\n---\\n\")\n",
    "\n",
    "        posts = []\n",
    "        for submission in redditor.submissions.new(limit=None):\n",
    "            posts.append(f\"Post in r/{submission.subreddit.display_name} (Score: {submission.score}): {submission.title}\\n{submission.selftext}\\n---\\n\")\n",
    "        \n",
    "        print(f\"Found {len(comments)} comments and {len(posts)} posts for user '{username}'.\")\n",
    "        \n",
    "        return {\n",
    "            \"comments\": comments, \n",
    "            \"posts\": posts,\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while scraping Reddit: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04e8e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_user_persona(user_data):\n",
    "    \"\"\"\n",
    "    Builds a user persona based on scraped Reddit data using the Google Gemini LLM,\n",
    "    with a strong emphasis on citing every piece of information.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        genai.configure(api_key=os.environ['GOOGLE_API_KEY'])\n",
    "    except Exception as e:\n",
    "        return f\"Google API Key configuration failed. Ensure you have entered it correctly. Error: {e}\"\n",
    "\n",
    "    if not user_data or (not user_data[\"comments\"] and not user_data[\"posts\"]):\n",
    "        return \"Could not generate a persona due to lack of user data.\"\n",
    "\n",
    "    all_content = \"\".join(user_data[\"comments\"]) + \"\".join(user_data[\"posts\"])\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    You are an expert analyst. Based on the following Reddit comments and posts, create a detailed user persona.\n",
    "    \n",
    "    **CRITICAL INSTRUCTION:** For every single point you make in every category, you MUST provide a direct quote from the user's content as a citation. The format is mandatory: (Citation: \"The user's original text snippet...\"). If you cannot find a direct quote to support a point and must make an inference, you MUST state it as (Citation: Inferred from overall activity). Do not make up citations.\n",
    "\n",
    "    **EXAMPLE OF THE REQUIRED FORMAT:**\n",
    "    *   **Interests:**\n",
    "        *   Baking Sourdough Bread (Citation: \"My starter is finally active enough to bake a proper sourdough loaf this weekend.\")\n",
    "        *   Classic Science Fiction (Citation: \"Just finished re-reading Dune for the fifth time, it never gets old.\")\n",
    "        *   PC Gaming (Citation: Inferred from frequent posts in r/buildapc and r/gaming.)\n",
    "\n",
    "    **--- START PERSONA GENERATION ---**\n",
    "\n",
    "    **Persona Title:** [Create a short, creative title for the persona, like \"The Knowledgeable Hobbyist\"]\n",
    "\n",
    "    **Demographics:**\n",
    "    *   **Age (Estimated):** [Estimate the age range] (Citation: ...)\n",
    "    *   **Location (Estimated):** [Estimate the location] (Citation: ...)\n",
    "    *   **Occupation (Estimated):** [Estimate the occupation] (Citation: ...)\n",
    "    *   **Interests:** [List key interests as bullet points, each with its own citation]\n",
    "\n",
    "    **Personality (based on user's tone and topics):**\n",
    "    *   [Provide a brief analysis of the user's personality, with a citation for each observation.]\n",
    "\n",
    "    **Motivations:**\n",
    "    *   [List user's key motivations as bullet points, each with its own citation.]\n",
    "\n",
    "    **Frustrations/Pain Points:**\n",
    "    *   [List user's key frustrations as bullet points, each with its own citation.]\n",
    "\n",
    "    **Quote:**\n",
    "    *   [A representative quote from the user's comments/posts that captures their essence.]\n",
    "\n",
    "    **Reddit Activity Summary:**\n",
    "    *   **Most Active Subreddits:** [List the subreddits the user is most active in] (Citation: Inferred from the source of the provided comments and posts.)\n",
    "    *   **General Tone:** [Describe the overall tone] (Citation: ...)\n",
    "\n",
    "    **--- END PERSONA GENERATION ---**\n",
    "\n",
    "    Here is the user's content (use this for your analysis):\n",
    "    ---\n",
    "    {all_content[:25000]}\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        model = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
    "        response = model.generate_content(prompt)\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        print(f\"Full error from Google AI: {e}\")\n",
    "        return f\"An error occurred with the Google AI API. Check the error message above.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d56bbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_persona_to_file(username, persona_text):\n",
    "    filename = f\"{username}_persona.txt\"\n",
    "    output_dir = 'output'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    filename = os.path.join(output_dir, f\"{username}_persona.txt\")\n",
    "    \n",
    "    cleaned_text = persona_text.replace('*', '')\n",
    "    try:\n",
    "        with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(cleaned_text)\n",
    "        print(f\"Cleaned persona successfully saved to {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not save the text file. Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77385e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def save_persona_to_json_final(username, persona_text):\n",
    "    \"\"\"\n",
    "    Parses the text persona and saves it to a uniquely named JSON file\n",
    "    directly inside the 'public' folder.\n",
    "    \"\"\"\n",
    "    persona_data = {\n",
    "        \"personaTitle\": \"N/A\", \"demographics\": {\"Age\": \"N/A\", \"Occupation\": \"N/A\", \"Location\": \"N/A\"},\n",
    "        \"habits\": [], \"frustrations\": [], \"goals\": [], \"quote\": \"\"\n",
    "    }\n",
    "    try:\n",
    "        \n",
    "        def get_section_content(title, text):\n",
    "            pattern = re.compile(r\"\\*\\*\" + re.escape(title) + r\":\\*\\*\\n(.*?)(?=\\n\\n\\*\\*|$)\", re.DOTALL)\n",
    "            match = pattern.search(text)\n",
    "            return match.group(1).strip() if match else \"\"\n",
    "        \n",
    "        # Helper for list items\n",
    "        def get_list_items(block_content):\n",
    "            if not block_content: return []\n",
    "            items = re.findall(r\"\\* (.*?)(?=\\s*\\(Citation:)\", block_content)\n",
    "            return [item.strip() for item in items]\n",
    "\n",
    "        title_match = re.search(r\"\\*\\*Persona Title:\\*\\*\\s*(.*)\", persona_text)\n",
    "        if title_match:\n",
    "            persona_data[\"personaTitle\"] = title_match.group(1).strip()\n",
    "\n",
    "        demographics_block = get_section_content(\"Demographics\", persona_text)\n",
    "        if demographics_block:\n",
    "            age_match = re.search(r\"Age \\(Estimated\\):\\*\\* (.*?)\\s*\\(Citation:\", demographics_block)\n",
    "            loc_match = re.search(r\"Location \\(Estimated\\):\\*\\* (.*?)\\s*\\(Citation:\", demographics_block)\n",
    "            occ_match = re.search(r\"Occupation \\(Estimated\\):\\*\\* (.*?)\\s*\\(Citation:\", demographics_block)\n",
    "            if age_match: persona_data[\"demographics\"][\"Age\"] = age_match.group(1).strip()\n",
    "            if loc_match: persona_data[\"demographics\"][\"Location\"] = loc_match.group(1).strip()\n",
    "            if occ_match: persona_data[\"demographics\"][\"Occupation\"] = occ_match.group(1).strip()\n",
    "\n",
    "        persona_data[\"habits\"] = get_list_items(get_section_content(\"Interests\", persona_text))\n",
    "        persona_data[\"frustrations\"] = get_list_items(get_section_content(\"Frustrations/Pain Points\", persona_text))\n",
    "        persona_data[\"goals\"] = get_list_items(get_section_content(\"Motivations\", persona_text))\n",
    "\n",
    "        quote_match = re.search(r\"\\*\\*Quote:\\*\\*\\s*\\n\\s*\\*.*?\\\"(.*?)\\\"\", persona_text, re.DOTALL)\n",
    "        if quote_match:\n",
    "            persona_data[\"quote\"] = quote_match.group(1).strip()\n",
    "        \n",
    "        output_dir = 'public'\n",
    "\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        filename = os.path.join(output_dir, f\"persona_data.json\")\n",
    "        \n",
    "        with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(persona_data, f, indent=4)\n",
    "        \n",
    "        print(f\"Definitive persona data successfully saved to {filename}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred in the FINAL JSON parser. Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c1d4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    reddit_url = input(\"Enter the full Reddit user profile URL: \")\n",
    "    \n",
    "    try:\n",
    "        username_part = reddit_url.split('/user/')[1]\n",
    "        reddit_username = username_part.split('/')[0]\n",
    "        print(f\"\\nExtracted username: '{reddit_username}'. Starting process...\")\n",
    "\n",
    "        print(\"\\nStep 1: Scraping data from Reddit...\")\n",
    "        user_data = scrape_redditor_data(reddit_username)\n",
    "\n",
    "        if user_data:\n",
    "            print(\"\\nStep 2: Building user persona with Google Gemini...\")\n",
    "            persona_text = build_user_persona(user_data)\n",
    "            \n",
    "            print(\"\\nStep 3: Saving persona to files...\")\n",
    "            save_persona_to_file(reddit_username, persona_text)\n",
    "            save_persona_to_json_final(reddit_username, persona_text)\n",
    "\n",
    "            print(\"----------------------------------\")\n",
    "            \n",
    "        else:\n",
    "            print(\"\\nProcess stopped because no data could be scraped for the user.\")\n",
    "\n",
    "    except IndexError:\n",
    "        print(\"\\n[ERROR] Invalid URL format. Please use a full Reddit user profile URL.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
